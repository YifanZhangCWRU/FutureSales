{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "### get raw data path from json header, then load all the raw data\r\n",
    "!pip install pandas\r\n",
    "from datetime import datetime\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "settings = json.load(open('settings.json'))\r\n",
    "settings\r\n",
    "# read csv\r\n",
    "item_categories = pd.read_csv(settings[\"DATA_DIR\"] + 'item_categories.csv')\r\n",
    "items = pd.read_csv(settings[\"DATA_DIR\"] + 'items.csv')\r\n",
    "shops = pd.read_csv(settings[\"DATA_DIR\"] + 'shops.csv')\r\n",
    "sales_train = pd.read_csv(settings[\"TRAIN_DATA_PATH\"],\r\n",
    "                          parse_dates=['date'],\r\n",
    "                          date_parser=lambda x: datetime.strptime(x, '%d.%m.%Y'))\r\n",
    "test = pd.read_csv(settings[\"TEST_DATA_PATH\"])\r\n",
    "# $pip install nltk\r\n",
    "# $pip install googletrans\r\n",
    "# please run in terminals if you don't have them\r\n",
    "# below are basics data processing/visualization"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Ivan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ivan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ivan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ivan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ivan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ivan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# below are housekeeping, timing ,etc kits\r\n",
    "import pickle\r\n",
    "import gc\r\n",
    "import re\r\n",
    "import string \r\n",
    "import tqdm\r\n",
    "import os\r\n",
    "\r\n",
    "# below are modeling \r\n",
    "import sklearn\r\n",
    "import lightgbm as lgb\r\n",
    "import catboost\r\n",
    "\r\n",
    "# below are NLP packages for text and google translate to translate russian into English\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('stopwords')\r\n",
    "nltk.download('wordnet')\r\n",
    "import gensim\r\n",
    "import googletrans\r\n",
    "import re\r\n",
    "\r\n",
    "# Below are the specific libraries in the packages\r\n",
    "from math import sqrt\r\n",
    "from numpy import loadtxt\r\n",
    "from itertools import product\r\n",
    "from tqdm import tqdm,tqdm_notebook\r\n",
    "from matplotlib import pyplot\r\n",
    "from googletrans import Translator\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler,StandardScaler\r\n",
    "from sklearn.feature_extraction import text\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,r2_score\r\n",
    "from sklearn.model_selection import KFold, train_test_split #StratifiedKFold\r\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\r\n",
    "from sklearn.decomposition import NMF #PCA, TruncatedSVD, \r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from catboost import CatBoostRegressor\r\n",
    "#from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\r\n",
    "\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.tokenize import MWETokenizer\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "#from nltk.stem import LancasterStemmer   # DO NOT USE- not reliable in this application\r\n",
    "#stemmer= LancasterStemmer()\r\n",
    "from itertools import tee, islice\r\n",
    "from collections import Counter\r\n",
    "from googletrans import Translator\r\n",
    "\r\n",
    "for p in [np, pd, sklearn, lgb]:\r\n",
    "    print (p.__name__, p.__version__)\r\n",
    "\r\n",
    "# downcast variables to save memory\r\n",
    "def downcast_dtypes(df):\r\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\r\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\r\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\r\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\r\n",
    "    return df\r\n",
    "\r\n",
    "# get most frequent words\r\n",
    "def get_top_n_words(corpus, n=None):\r\n",
    "    \"\"\"\r\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\r\n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \r\n",
    "    # https://gist.github.com/CristhianBoujon/c719ba2287a630a6d3821d37a9608ac8\r\n",
    "    \"\"\"\r\n",
    "    vec = CountVectorizer().fit(corpus)\r\n",
    "    bag_of_words = vec.transform(corpus)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:n]\r\n",
    "\r\n",
    "# get ngrams\r\n",
    "def ngrams(lst, n):\r\n",
    "  tlst = lst\r\n",
    "  while True:\r\n",
    "    a, b = tee(tlst)\r\n",
    "    l = tuple(islice(a, n))\r\n",
    "    if len(l) == n:\r\n",
    "      yield l\r\n",
    "      next(b)\r\n",
    "      tlst = b\r\n",
    "    else:\r\n",
    "        if len(l)==1:\r\n",
    "          yield lst\r\n",
    "          tlst = lst\r\n",
    "          break\r\n",
    "        else:\r\n",
    "          break\r\n",
    " # from https://stackoverflow.com/questions/12488722/counting-bigrams-pair-of-two-words-in-a-file-using-python\r\n",
    " # with modifications to trap errors in case sentence has only 1 word\r\n",
    "\r\n",
    "def duplicate_removal(w):\r\n",
    "    word_list = []\r\n",
    "    [word_list.append(s) for s in w if s not in word_list]\r\n",
    "    return word_list\r\n",
    "#https://www.engineeringbigdata.com/python-exercise-10-remove-duplicate-words-with-user-input/   this removes duplicated words in a sentence"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numpy 1.21.2\n",
      "pandas 1.3.3\n",
      "sklearn 1.0\n",
      "lightgbm 3.3.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Setting up text - to - numeric - feature extraction code\r\n",
    "# These are the routine customized methods for processing . here since categorical features for shop_name and category_name were less than 100,\r\n",
    "# one can run these without much performance issues. I do not recommend using this for item_names as it has 22170 unique objects\r\n",
    "tokenizer = MWETokenizer()\r\n",
    "tokenizer.add_mwe(('xbox','one'))\r\n",
    "tokenizer.add_mwe(('xbox','360'))\r\n",
    "unwantedtokens=[':',',','(',')','-','.','!','/']\r\n",
    "translator = Translator()\r\n",
    "# google translate from Russian to English\r\n",
    "\r\n",
    "def get_ngrams(text,n):\r\n",
    "    n_grams = ngrams(tokenizer.tokenize((text).split()), n)\r\n",
    "    return [ ' '.join(grams) for grams in n_grams]\r\n",
    "    \r\n",
    "def text_cleaning(text_df):\r\n",
    "    # process the text in this mini routine\r\n",
    "\t# step1: translate using google translate\r\n",
    "\t# step2: lemmatize , gaming-game, xbox 360 with underscores\r\n",
    "\t# step3: remove dash, replace by underscores for two-words; then tokenize\r\n",
    "\t# step4: stem words (optional)\r\n",
    "    for sentence in range(text_df.index.stop):\r\n",
    "        temp=translator.translate(text_df[sentence],dest='en')\r\n",
    "        testb=temp.text\r\n",
    "        del temp\r\n",
    "        # change gaming to game, blue-ray to blue_ray(to make sure it detect it as one word)\r\n",
    "        temp=testb.replace('ing', 'e') \r\n",
    "        testb=temp\r\n",
    "        del temp\r\n",
    "        ngram2=get_ngrams(testb,2)\r\n",
    "        # make xbox_360 and x_box_one etc\r\n",
    "        testb=(\" \").join(ngram2)   \r\n",
    "        # remove tokens, then remove duplicates, lemmatize and stem (stem is not implemented today)\r\n",
    "        text_tokens = word_tokenize(testb)\r\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\r\n",
    "        filtered_sentence = [word for word in tokens_without_sw if not word in unwantedtokens]\r\n",
    "        clean_sentence=\" \".join(duplicate_removal(filtered_sentence))\r\n",
    "        word_list = nltk.word_tokenize(clean_sentence)\r\n",
    "        lemmatized_sentence = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\r\n",
    "        temp=lemmatized_sentence.replace('-', '_')  \r\n",
    "        # change blue-ray to blue_ray to make sure it is treated as one word; since the stand-alone '-' has been removed, in-words '-' should be converted to '_'\r\n",
    "        stemmed_sentence=temp\r\n",
    "        del temp\r\n",
    "        #stemmed_sentence= stemmer.stem(lemmatized_sentence)\r\n",
    "        testa[sentence]=stemmed_sentence\r\n",
    "    return text_df\r\n",
    "\r\n",
    "def plot_top_words(text_df,num_top_words):\r\n",
    "    common_words = get_top_n_words(text_df, num_top_words)\r\n",
    "    # picked top 40 words, because 40 will start to have common_words with minimum frequency of 1 - trivial solution\r\n",
    "    # you can modify it and pass it as argument instead\r\n",
    "    for word, freq in common_words:\r\n",
    "        print(word, freq)\r\n",
    "    # for 'top words'. \r\n",
    "    keywords,frequency=zip(*common_words)\r\n",
    "    fig, ax = plt.subplots()\r\n",
    "    fig.set_size_inches(9,9)\r\n",
    "    plt.title('keywords and frequency of appearance ')\r\n",
    "    y_pos = np.arange(len(keywords))\r\n",
    "    plt.barh(y_pos, frequency)\r\n",
    "    plt.yticks(y_pos, keywords)\r\n",
    "    plt.show()\r\n",
    "    return\r\n",
    "\r\n",
    "def text_tfidf_num(text_df):\r\n",
    "    countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\r\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\r\n",
    "    count_wm = countvectorizer.fit_transform(text_df)\r\n",
    "    tfidf_wm = tfidfvectorizer.fit_transform(text_df)\r\n",
    "    count_tokens = countvectorizer.get_feature_names()\r\n",
    "    tfidf_tokens = tfidfvectorizer.get_feature_names()\r\n",
    "    df_countvect = pd.DataFrame(data = count_wm.toarray(),columns = count_tokens)\r\n",
    "    df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\r\n",
    "    print(\"Count Vectorizer\\n\")\r\n",
    "    print(df_countvect.head(5))\r\n",
    "    print(\"\\nTD-IDF Vectorizer\\n\")\r\n",
    "    print(df_tfidfvect.head(5))\r\n",
    "    return df_tfidfvect"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "testa=item_categories.item_category_name\r\n",
    "testb=text_cleaning(testa)\r\n",
    "plot_top_words(testb,40)\r\n",
    "item_cat_num=text_tfidf_num(testb)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "   1c  360  3d  4k  accessories  additional  albums  android  artbooks  \\\n",
      "0   0    0   0   0            0           0       0        0         0   \n",
      "1   0    0   0   0            1           0       0        0         0   \n",
      "2   0    0   0   0            1           0       0        0         0   \n",
      "3   0    0   0   0            1           0       0        0         0   \n",
      "4   0    0   0   0            1           0       0        0         0   \n",
      "\n",
      "   attribute  ...  standard  stickers  tickets  toys  traine  video  vinyl  \\\n",
      "0          0  ...         0         0        0     0       0      0      0   \n",
      "1          0  ...         0         0        0     0       0      0      0   \n",
      "2          0  ...         0         0        0     0       0      0      0   \n",
      "3          0  ...         0         0        0     0       0      0      0   \n",
      "4          0  ...         0         0        0     0       0      0      0   \n",
      "\n",
      "   windows  xbox  игры  \n",
      "0        0     0     0  \n",
      "1        0     0     0  \n",
      "2        0     0     0  \n",
      "3        0     0     0  \n",
      "4        0     0     0  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n",
      "    1c  360   3d   4k  accessories  additional  albums  android  artbooks  \\\n",
      "0  0.0  0.0  0.0  0.0     0.000000         0.0     0.0      0.0       0.0   \n",
      "1  0.0  0.0  0.0  0.0     0.624735         0.0     0.0      0.0       0.0   \n",
      "2  0.0  0.0  0.0  0.0     0.624735         0.0     0.0      0.0       0.0   \n",
      "3  0.0  0.0  0.0  0.0     0.624735         0.0     0.0      0.0       0.0   \n",
      "4  0.0  0.0  0.0  0.0     0.624735         0.0     0.0      0.0       0.0   \n",
      "\n",
      "   attribute  ...  standard  stickers  tickets  toys  traine  video  vinyl  \\\n",
      "0        0.0  ...       0.0       0.0      0.0   0.0     0.0    0.0    0.0   \n",
      "1        0.0  ...       0.0       0.0      0.0   0.0     0.0    0.0    0.0   \n",
      "2        0.0  ...       0.0       0.0      0.0   0.0     0.0    0.0    0.0   \n",
      "3        0.0  ...       0.0       0.0      0.0   0.0     0.0    0.0    0.0   \n",
      "4        0.0  ...       0.0       0.0      0.0   0.0     0.0    0.0    0.0   \n",
      "\n",
      "   windows  xbox  игры  \n",
      "0      0.0   0.0   0.0  \n",
      "1      0.0   0.0   0.0  \n",
      "2      0.0   0.0   0.0  \n",
      "3      0.0   0.0   0.0  \n",
      "4      0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "item_category_name.info()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'item_category_name' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15696/1146392938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mitem_category_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'item_category_name' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "63d64d53abb64f4c31c9f8aed18c735b290ce78d4469bb66fc945cce61490c3b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}